# ðŸ¤– Custom AI Model Implementation Guide

## Overview

Your Streamlit app now supports custom AI models instead of requiring OpenAI API keys. This guide shows you how to implement your own AI explanation model.

## ðŸŽ¯ Current Implementation

The app now uses an **Enhanced Rule-Based AI** model that provides intelligent explanations without external API calls. Features include:

- âœ… **No API key required**
- âœ… **Privacy-focused** (no external calls)
- âœ… **Domain-specific knowledge**
- âœ… **Statistical analysis**
- âœ… **Error pattern recognition**
- âœ… **Severity assessment**

## ðŸ”§ How to Implement Your Own AI Model

### Step 1: Create Your Custom AI Model

Replace the `YourCustomAIModel` class in `utils/custom_ai_model.py`:

```python
class YourCustomAIModel(CustomAIModel):
    def __init__(self, model_path: str = None):
        # Initialize your AI model here
        self.model = self._load_model(model_path)
        self.available = True
    
    def _load_model(self, model_path: str):
        # Load your AI model (e.g., local LLM, fine-tuned model, etc.)
        # Example: return transformers.AutoModel.from_pretrained(model_path)
        pass
    
    def is_available(self) -> bool:
        return self.available
    
    def generate_answer(
        self, 
        question: str, 
        metrics_df: pd.DataFrame, 
        logs_df: pd.DataFrame,
        **kwargs
    ) -> Dict[str, str]:
        # Your AI model logic here
        try:
            # 1. Preprocess the data
            processed_data = self._preprocess_data(question, metrics_df, logs_df)
            
            # 2. Generate response using your model
            response = self.model.generate(processed_data)
            
            # 3. Post-process the output
            answer, reasoning = self._postprocess_response(response)
            
            return {"answer": answer, "reasoning": reasoning}
            
        except Exception as e:
            return {
                "answer": "Custom AI model encountered an error.",
                "reasoning": f"Error: {str(e)}"
            }
```

### Step 2: Update the Model Factory

Modify the `get_ai_model()` function in `utils/custom_ai_model.py`:

```python
def get_ai_model() -> CustomAIModel:
    """Get the AI model to use for explanations."""
    # Return your custom model instead of EnhancedRuleBasedAI
    return YourCustomAIModel(model_path="path/to/your/model")
```

## ðŸš€ Example Implementations

### Option 1: Local LLM (e.g., Llama, Mistral)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class LocalLLMAI(CustomAIModel):
    def __init__(self, model_path: str = "microsoft/DialoGPT-medium"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_path)
        self.available = True
    
    def generate_answer(self, question, metrics_df, logs_df, **kwargs):
        # Create context from metrics and logs
        context = self._create_context(question, metrics_df, logs_df)
        
        # Generate response
        inputs = self.tokenizer.encode(context, return_tensors="pt")
        outputs = self.model.generate(inputs, max_length=200, temperature=0.7)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {"answer": response, "reasoning": "Generated by local LLM"}
```

### Option 2: Fine-tuned Model

```python
import joblib
from sklearn.ensemble import RandomForestClassifier

class FineTunedAI(CustomAIModel):
    def __init__(self, model_path: str = "models/observability_classifier.pkl"):
        self.model = joblib.load(model_path)
        self.available = True
    
    def generate_answer(self, question, metrics_df, logs_df, **kwargs):
        # Extract features
        features = self._extract_features(metrics_df, logs_df)
        
        # Predict issue type
        issue_type = self.model.predict([features])[0]
        
        # Generate explanation based on prediction
        answer = self._generate_explanation(issue_type, metrics_df, logs_df)
        
        return {"answer": answer, "reasoning": f"Classified as: {issue_type}"}
```

### Option 3: Rule-Based Expert System

```python
class ExpertSystemAI(CustomAIModel):
    def __init__(self):
        self.rules = self._load_expert_rules()
        self.available = True
    
    def generate_answer(self, question, metrics_df, logs_df, **kwargs):
        # Apply expert rules
        diagnosis = self._apply_rules(metrics_df, logs_df)
        
        # Generate explanation
        answer = self._explain_diagnosis(diagnosis)
        
        return {"answer": answer, "reasoning": f"Applied {len(self.rules)} expert rules"}
```

## ðŸ”„ Integration with Streamlit App

The app automatically uses your custom AI model:

1. **No code changes needed** in `app.py`
2. **Model selection** in the sidebar
3. **Automatic fallback** to OpenAI if needed
4. **Same interface** for all AI models

## ðŸ“Š Data Available to Your Model

Your AI model receives:

### Metrics DataFrame:
- `timestamp`: Time of measurement
- `cpu_usage`: CPU percentage
- `memory_usage`: Memory percentage  
- `latency_ms`: Response time in milliseconds

### Logs DataFrame:
- `timestamp`: Time of log entry
- `level`: Log level (INFO, WARN, ERROR)
- `message`: Log message content

### Question Context:
- User's natural language question
- Current system state
- Historical patterns

## ðŸŽ¯ Best Practices

### 1. **Error Handling**
```python
def generate_answer(self, question, metrics_df, logs_df, **kwargs):
    try:
        # Your AI logic here
        return {"answer": response, "reasoning": reasoning}
    except Exception as e:
        return {
            "answer": "AI model encountered an error.",
            "reasoning": f"Error: {str(e)}"
        }
```

### 2. **Data Validation**
```python
def _validate_inputs(self, metrics_df, logs_df):
    if metrics_df.empty or logs_df.empty:
        raise ValueError("Empty data provided")
    # Add more validation as needed
```

### 3. **Performance Optimization**
```python
def __init__(self, model_path: str = None):
    # Load model once during initialization
    self.model = self._load_model(model_path)
    self.cache = {}  # Add caching if needed
```

## ðŸ§ª Testing Your Model

Create a test script:

```python
# test_my_ai.py
from utils.custom_ai_model import get_ai_model
import pandas as pd

# Load data
metrics_df = pd.read_csv('data/metrics.csv', parse_dates=['timestamp'])
logs_df = pd.read_csv('data/logs.csv', parse_dates=['timestamp'])

# Test your model
ai_model = get_ai_model()
result = ai_model.generate_answer("Why is CPU usage high?", metrics_df, logs_df)
print(f"Answer: {result['answer']}")
print(f"Reasoning: {result['reasoning']}")
```

## ðŸš€ Deployment

1. **Save your model** in the `utils/` directory
2. **Update** `get_ai_model()` to return your model
3. **Test** with the provided test script
4. **Deploy** - your Streamlit app will automatically use your model

## âœ… Benefits of Custom AI Models

- **Privacy**: No data sent to external APIs
- **Control**: Full control over model behavior
- **Cost**: No API costs
- **Customization**: Domain-specific knowledge
- **Reliability**: No dependency on external services
- **Speed**: Local inference is faster

Your custom AI model is now ready to provide intelligent explanations about system health without requiring any external API keys! 